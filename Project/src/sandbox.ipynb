{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.transformer import Transformer\n",
    "from options.base_options import BaseOptions\n",
    "from models.Transform2D import Transform2D\n",
    "from datasets.shape_net import ShapeNet\n",
    "import torch\n",
    "from blocks.patch_encoder import PatchEncoder\n",
    "from einops import rearrange\n",
    "from tqdm.notebook import tqdm\n",
    "from models.Transform2D import Transform2D\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ShapeNet(cat=\"airplane\")\n",
    "len(dataset)\n",
    "train_ds = dataset\n",
    "model = Transform2D()\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=16,   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b307112d99c1472eb6417dd9a0328bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/mnt/hdd/streakfull/3DML/Project/src/blocks/simple_decoder.py\u001b[0m(37)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     35 \u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_transpose3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_transpose4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 37 \u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x.shape\n",
      "torch.Size([16, 1, 32, 32, 32])\n",
      "ipdb> qu8it\n",
      "*** NameError: name 'qu8it' is not defined\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizations import visualize_images\n",
    "\n",
    "#import pdb;pdb.set_trace()\n",
    "#images = rearrange(images, 'bs c d w -> bs d w c')\n",
    "visualize_images(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = rearrange(images, 'bs c d w -> bs d w c')\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape\n",
    "images.shape\n",
    "images[0][:,:,0]\n",
    "images.dtype\n",
    "y.dtype\n",
    "# y = y.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.randn(1, 4, 137, 137)\n",
    "kc, kh, kw = 4, 16, 16  # kernel size\n",
    "dc, dh, dw = 1, 1, 1  # stride\n",
    "# Pad to multiples of 32\n",
    "x = F.pad(x, (x.size(2)%kw // 2, x.size(2)%kw // 2,\n",
    "              x.size(1)%kh // 2, x.size(1)%kh // 2,\n",
    "              x.size(0)%kc // 2, x.size(0)%kc // 2))\n",
    "\n",
    "patches = x.unfold(1, kc, dc).unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "unfold_shape = patches.size()\n",
    "patches = patches.contiguous().view(-1, kc, kh, kw)\n",
    "print(patches.shape)\n",
    "\n",
    "# Reshape back\n",
    "patches_orig = patches.view(unfold_shape)\n",
    "output_c = unfold_shape[1] * unfold_shape[4]\n",
    "output_h = unfold_shape[2] * unfold_shape[5]\n",
    "output_w = unfold_shape[3] * unfold_shape[6]\n",
    "patches_orig = patches_orig.permute(0, 1, 4, 2, 5, 3, 6).contiguous()\n",
    "patches_orig = patches_orig.view(1, output_c, output_h, output_w)\n",
    "\n",
    "patches_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.model_utils import summarize_model\n",
    "total_params = sum(p.numel() for p in model.transformer.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.ones((3,5,5,5,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "x[0][0][0][0]\n",
    "y = rearrange(x, 'bs c1 c2 c3 c4 -> bs c4 c1 c2 c3 ')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n",
    "sfy = torch.nn.Softmax(dim=1)\n",
    "sfx = torch.nn.Softmax(dim=-1)\n",
    "y2 = sfy(y)\n",
    "x2 = sfx(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2\n",
    "x2\n",
    "y3 = rearrange(y2,'bs c4 c1 c2 c3 ->bs c1 c2 c3 c4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3==x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_deconvfilter = [128, 128, 128, 64, 32, 2]\n",
    "filter_shape = (n_deconvfilter[1], 3, 3, 3)\n",
    "filter_shape = (n_deconvfilter[3], 3, 3, 3)\n",
    "padding = [0, int((filter_shape[1] - 1) / 2), 0, int((filter_shape[2] - 1) / 2),\n",
    "                             int((filter_shape[3] - 1) / 2) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import __add__\n",
    "kernel_sizes = (16, 16)\n",
    "conv_padding = reduce(__add__, \n",
    "    [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in kernel_sizes[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange,repeat\n",
    "import torch\n",
    "embedd = torch.tensor([[1,2,3],[4,5,6]])\n",
    "embedd.shape\n",
    "pos = torch.tensor([[10,10,10]])\n",
    "pos.shape\n",
    "positions =  repeat(pos,'n d -> (repeat n) d', repeat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
