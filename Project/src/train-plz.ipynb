{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import mkdir,seed_all\n",
    "from omegaconf import OmegaConf\n",
    "from cprint import *\n",
    "from datasets.shape_net import ShapeNet\n",
    "import torch\n",
    "from models.Transform2D import Transform2D\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1b5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m{'logs_dir': 'logs', 'is_train': True, 'name': 'testingOverfit', 'device': 'cuda:0', 'batch_size': 8, 'n_epochs': 500, 'print_every': 300, 'validate_every': 300, 'model': {'lr': 0.0001, 'criterion': 'BCE', 'pos_weight': 1.3, 'encoder': {'patch_size': 13, 'sequence_length': 100, 'embedding_dim': 768, 'patch_padding': 3}, 'transformer_encoder': {'d_model': 768, 'nhead': 12, 'num_layers': 12}, 'transformer_decoder': {'d_model': 768, 'nhead': 12, 'num_layers': 8, 'num_pos_embeddings': 64}}}\u001b[0m\n",
      "\u001b[94m- logs/all_dataset directory found\u001b[0m\n",
      "\u001b[94m- logs/all_dataset/testingOverfit directory found\u001b[0m\n",
      "\u001b[94m- logs/all_dataset/testingOverfit/checkpoints directory found\u001b[0m\n",
      "\u001b[94m- logs/all_dataset/testingOverfit/tb directory found\u001b[0m\n",
      "\u001b[94m- logs/all_dataset/testingOverfit/visuals directory found\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "seed_all(111)\n",
    "config = OmegaConf.load(\"./configs/global_configs.yaml\")\n",
    "cprint.ok(config)\n",
    "description = \"Testing Overfit transforemer3\" # Describe Experiment params here\n",
    "logs_dir = \"logs/all_dataset\"\n",
    "mkdir(logs_dir)\n",
    "\n",
    "experiment_dir = f\"{logs_dir}/{config['name']}\"\n",
    "mkdir(experiment_dir)\n",
    "mkdir(f\"{experiment_dir}/checkpoints\")\n",
    "mkdir(f\"{experiment_dir}/tb\")\n",
    "mkdir(f\"{experiment_dir}/visuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b782196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43783"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ShapeNet(cat=\"all\",is_overfit=False) #Change overfit param here & cat here\n",
    "# train_ds, valid_ds, test_ds = torch.utils.data.random_split(\n",
    "#     dataset, [64, 0, 0])\n",
    "train_ds = dataset\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=16,   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "len(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06198817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mUsing device:\u001b[0m cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transform2D(\n",
       "  (patch_encoder): PatchEncoder(\n",
       "    (pos_embedding): Embedding(100, 768)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(4, 768, kernel_size=(13, 13), stride=(13, 13))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (net): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (net): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embeddings): Embedding(64, 768)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): SimpleDecoder(\n",
       "    (initial_conv): Conv3d(768, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (encoded_conv): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_transpose): Sequential(\n",
       "      (0): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv_transpose4): ConvTranspose3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transform2D()\n",
    "# Declare device\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available() and config['device'].startswith('cuda'):\n",
    "    device = torch.device(config['device'])\n",
    "    cprint.ok('Using device:', config['device'])\n",
    "else:\n",
    "    cprint.warn('Using CPU')\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900412ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5b007aa134ff09863e8b05d3cea29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8feb76367b4b968b363d3c95e51074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mModel Loss end of batch 0: 0.8294427394866943\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 20: 0.4925037622451782\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 40: 0.381027489900589\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 60: 0.35424137115478516\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 80: 0.39149755239486694\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 100: 0.3078804612159729\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 120: 0.3874923288822174\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 140: 0.3652151823043823\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 160: 0.3390806019306183\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 180: 0.3675973415374756\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 200: 0.20725247263908386\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 220: 0.6718246936798096\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 240: 0.3547462821006775\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 260: 0.25616690516471863\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 280: 0.3696037828922272\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 300: 0.3539610207080841\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 320: 0.7379753589630127\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 340: 0.45155563950538635\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 360: 0.35715973377227783\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 380: 0.30028027296066284\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 400: 0.34281522035598755\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 420: 0.37112802267074585\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 440: 0.44004982709884644\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 460: 0.3333306908607483\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 480: 0.7472251653671265\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 500: 0.3169434666633606\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 520: 0.3689342737197876\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 540: 0.5134936571121216\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 560: 0.2657264769077301\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 580: 0.44200247526168823\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 600: 0.31708046793937683\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 620: 0.25957489013671875\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 640: 0.3774593770503998\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 660: 0.3485569953918457\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 680: 0.2744413912296295\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 700: 0.3409655690193176\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 720: 0.3394656479358673\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 740: 0.37902069091796875\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 760: 0.3368918299674988\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 780: 0.3336600065231323\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 800: 0.3639969825744629\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 820: 0.3731066882610321\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 840: 0.29891419410705566\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 860: 0.40485426783561707\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 880: 0.3610697388648987\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 900: 0.353689968585968\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 920: 0.3962348699569702\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 940: 0.36549633741378784\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 960: 0.2757419943809509\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 980: 0.3156753182411194\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1000: 0.3679942786693573\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1020: 0.47867053747177124\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1040: 0.2856370210647583\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1060: 0.2710961103439331\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1080: 0.3317486047744751\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1100: 0.38708412647247314\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1120: 0.5081211924552917\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1140: 0.38702553510665894\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1160: 0.37890613079071045\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1180: 0.34834378957748413\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1200: 0.41870197653770447\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1220: 0.3970739543437958\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1240: 0.3243207335472107\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1260: 0.3406709134578705\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1280: 0.298061728477478\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1300: 0.4672417640686035\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1320: 0.7337256669998169\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1340: 0.4336429536342621\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1360: 0.4920561909675598\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1380: 0.3086628317832947\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1400: 0.41154810786247253\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1420: 0.3666837215423584\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1440: 0.38854533433914185\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1460: 0.3611301779747009\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1480: 0.4810575842857361\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1500: 0.4172822833061218\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1520: 0.4104445278644562\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1540: 0.3182574510574341\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1560: 0.28099167346954346\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1580: 0.2499353289604187\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1600: 0.3453822135925293\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1620: 0.314510703086853\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1640: 0.3989750146865845\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1660: 0.34274789690971375\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1680: 0.35811957716941833\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1700: 0.40091538429260254\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1720: 0.33042052388191223\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1740: 0.3093266785144806\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1760: 0.5574848651885986\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1780: 0.34791865944862366\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1800: 0.2667187452316284\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1820: 0.4081640839576721\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1840: 0.3721659183502197\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1860: 0.6585230231285095\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1880: 0.25632861256599426\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1900: 0.3109496235847473\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1920: 0.3811914324760437\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1940: 0.29770898818969727\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1960: 0.3373543918132782\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1980: 0.5159153938293457\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2000: 0.21726584434509277\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2020: 0.48445776104927063\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2040: 0.21795812249183655\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2060: 0.36906740069389343\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2080: 0.3755408227443695\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2100: 0.4078366160392761\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2120: 0.48544129729270935\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2140: 0.2813343405723572\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2160: 0.4353342056274414\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2180: 0.3766595423221588\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2200: 0.3204928934574127\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2220: 0.30937331914901733\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2240: 0.31200021505355835\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2260: 0.38206231594085693\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2280: 0.29235678911209106\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2300: 0.4938943088054657\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2320: 0.5006946325302124\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2340: 0.3388231694698334\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2360: 0.4738903045654297\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2380: 0.3432048261165619\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2400: 0.4642539620399475\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2420: 0.35673725605010986\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2440: 0.3405419588088989\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2460: 0.3365371525287628\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2480: 0.47521865367889404\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2500: 0.49812814593315125\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2520: 0.28737902641296387\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2540: 0.32750022411346436\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2560: 0.4819639325141907\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2580: 0.40952426195144653\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2600: 0.2866532504558563\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2620: 0.36723363399505615\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2640: 0.36689138412475586\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2660: 0.3053380846977234\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2680: 0.3716137707233429\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2700: 0.49326953291893005\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2720: 0.33316531777381897\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mRunning Loss end of epoch 0: 1046.738037109375\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075a72a07c04460dbf437c111bb5f655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mModel Loss end of batch 0: 0.3664973974227905\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 20: 0.4962030351161957\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 40: 0.42741858959198\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 60: 0.28273627161979675\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 80: 0.45786580443382263\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 100: 0.2718019485473633\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 120: 0.3734484314918518\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 140: 0.3055804669857025\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 160: 0.29980945587158203\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 180: 0.39144498109817505\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 200: 0.2812526822090149\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 220: 0.48421478271484375\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 240: 0.6616445779800415\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 260: 0.48776888847351074\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 280: 0.36006903648376465\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 300: 0.34083840250968933\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 320: 0.3194693922996521\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 340: 0.3468107283115387\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 360: 0.45324569940567017\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 380: 0.47662264108657837\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 400: 0.28075918555259705\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 420: 0.28556370735168457\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 440: 0.3203686773777008\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 460: 0.30777108669281006\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 480: 0.4590839743614197\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 500: 0.4051816761493683\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 520: 0.5164098739624023\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 540: 0.367529034614563\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 560: 0.26360857486724854\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 580: 0.3964356780052185\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 600: 0.26643848419189453\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 620: 0.28117120265960693\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 640: 0.3298589587211609\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 660: 0.27351897954940796\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 680: 0.3275197446346283\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 700: 0.34945687651634216\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 720: 0.3751591742038727\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 740: 0.26632869243621826\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 760: 0.3746988773345947\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 780: 0.3593014180660248\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 800: 0.27423667907714844\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 820: 0.35284754633903503\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 840: 0.2953606843948364\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 860: 0.2621079683303833\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 880: 0.4600145220756531\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 900: 0.470436692237854\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 920: 0.4394630193710327\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 940: 0.397606760263443\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 960: 0.3058369755744934\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 980: 0.3876858353614807\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1000: 0.5982726812362671\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1020: 0.3442297875881195\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1040: 0.30312684178352356\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1060: 0.36512768268585205\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1080: 0.39127346873283386\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1100: 0.3396243155002594\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1120: 0.2998167872428894\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1140: 0.25943511724472046\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1160: 0.45718657970428467\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1180: 0.5181012153625488\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1200: 0.29296860098838806\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1220: 0.2793763279914856\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1240: 0.5019824504852295\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1260: 0.3442586660385132\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1280: 0.5109032392501831\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1300: 0.32020455598831177\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1320: 0.4078996777534485\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1340: 0.28860557079315186\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1360: 0.30229702591896057\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1380: 0.2991858720779419\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1400: 0.30940115451812744\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1420: 0.31764641404151917\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1440: 0.38007813692092896\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1460: 0.34129777550697327\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1480: 0.5427744388580322\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1500: 0.45564332604408264\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1520: 0.37378954887390137\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1540: 0.26236212253570557\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1560: 0.40306520462036133\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1580: 0.28240305185317993\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1600: 0.4064989686012268\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1620: 0.49217915534973145\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1640: 0.32606518268585205\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1660: 0.3824613690376282\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1680: 0.4403660297393799\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1700: 0.5106492638587952\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1720: 0.36926883459091187\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1740: 0.3111770451068878\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1760: 0.35222625732421875\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1780: 0.26706966757774353\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1800: 0.32068002223968506\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1820: 0.4116142988204956\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1840: 0.2578974664211273\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1860: 0.4359184503555298\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1880: 0.5070154070854187\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1900: 0.47287648916244507\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1920: 0.3312323987483978\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1940: 0.354013055562973\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1960: 0.3775850832462311\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1980: 0.3381580710411072\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2000: 0.28715652227401733\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2020: 0.28035926818847656\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2040: 0.35797181725502014\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2060: 0.2839728593826294\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2080: 0.4082998037338257\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2100: 0.4110572934150696\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2120: 0.4124460220336914\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2140: 0.31840386986732483\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2160: 0.4065663516521454\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2180: 0.42182496190071106\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2200: 0.3608008623123169\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2220: 0.3323419988155365\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2240: 0.2784839868545532\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2260: 0.4357219636440277\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2280: 0.4203770160675049\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2300: 0.6026906371116638\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2320: 0.5550110340118408\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2340: 0.3491981625556946\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2360: 0.4672930836677551\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2380: 0.4022279977798462\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2400: 0.38805675506591797\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2420: 0.4369976818561554\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2440: 0.4144690930843353\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2460: 0.5701417326927185\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2480: 0.29596731066703796\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2500: 0.42079105973243713\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2520: 0.3675938844680786\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2540: 0.41612887382507324\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2560: 0.3429422080516815\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2580: 0.3625491261482239\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2600: 0.3328624367713928\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2620: 0.3997974395751953\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2640: 0.38177490234375\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2660: 0.39255839586257935\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2680: 0.330382376909256\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2700: 0.41225141286849976\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 2720: 0.3306940197944641\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mRunning Loss end of epoch 1: 1028.63916015625\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1b95cb641b4fb79c047b79924f84f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mModel Loss end of batch 0: 0.3586563169956207\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 20: 0.2902850806713104\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 40: 0.3415076732635498\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 60: 0.5392053127288818\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 80: 0.3725860118865967\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 100: 0.38801923394203186\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 120: 0.4444429278373718\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 140: 0.3638705313205719\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 160: 0.54361891746521\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 180: 0.41215574741363525\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 200: 0.4024531841278076\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 220: 0.40993231534957886\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 240: 0.5018194913864136\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 260: 0.2549724578857422\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 280: 0.484999418258667\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 300: 0.26349005103111267\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 320: 0.3125271797180176\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 340: 0.39727556705474854\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 360: 0.4945871829986572\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 380: 0.29704439640045166\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 400: 0.45693540573120117\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 420: 0.5894832015037537\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 440: 0.3389415144920349\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 460: 0.4141496419906616\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 480: 0.3245031237602234\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 500: 0.35137277841567993\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 520: 0.2982063889503479\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 540: 0.2981894612312317\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 560: 0.3648667633533478\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 580: 0.3547290563583374\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 600: 0.3626829981803894\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 620: 0.34976357221603394\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 640: 0.3776925206184387\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 660: 0.5424089431762695\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 680: 0.3448517322540283\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 700: 0.4123866558074951\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 720: 0.41624706983566284\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 740: 0.33032768964767456\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 760: 0.32232534885406494\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 780: 0.3156030774116516\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 800: 0.33822178840637207\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 820: 0.45598888397216797\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 840: 0.3705138564109802\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 860: 0.31780797243118286\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 880: 0.3642442524433136\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 900: 0.3329477906227112\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 920: 0.31417980790138245\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 940: 0.41407260298728943\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 960: 0.4534547030925751\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 980: 0.272457093000412\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1000: 0.3585420548915863\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1020: 0.42137381434440613\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1040: 0.3406675457954407\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1060: 0.36288148164749146\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1080: 0.34205788373947144\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1100: 0.2932344377040863\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1120: 0.3569743037223816\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1140: 0.3836062550544739\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1160: 0.26044538617134094\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1180: 0.30403822660446167\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1200: 0.5154868364334106\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1220: 0.3221645951271057\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1240: 0.4168747067451477\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1260: 0.3269755244255066\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1280: 0.2989252507686615\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1300: 0.24722325801849365\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1320: 0.3206850290298462\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1340: 0.3717123866081238\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1360: 0.3624970316886902\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1380: 0.32007554173469543\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1400: 0.32618093490600586\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1420: 0.3320420980453491\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1440: 0.3733857274055481\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1460: 0.4571911096572876\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1480: 0.2863612771034241\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1500: 0.376171737909317\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1520: 0.4437032639980316\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1540: 0.47718101739883423\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1560: 0.2621317505836487\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1580: 0.3609769940376282\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1600: 0.587986946105957\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1620: 0.3203336000442505\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1640: 0.35991278290748596\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1660: 0.45705804228782654\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1680: 0.296362042427063\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1700: 0.2603995203971863\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1720: 0.26450878381729126\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1740: 0.35579121112823486\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1760: 0.2443961203098297\u001b[0m\n",
      "\u001b[93mModel Loss end of batch 1780: 0.287635862827301\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "\n",
    "runningloss=0;\n",
    "for epoch in tqdm(range(70)):\n",
    "     for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "          ShapeNet.move_batch_to_device(batch, device)\n",
    "          model.step(batch)\n",
    "          loss = model.get_loss()  \n",
    "          runningloss += loss[\"loss\"]\n",
    "          if(batch_idx %20 ==0):\n",
    "            loss = model.get_loss()\n",
    "            cprint.warn(f'Model Loss end of batch {batch_idx}: {loss[\"loss\"]}')\n",
    "            #cprint.warn(f'Model Demo Loss end of batch {batch_idx}: {loss[\"loss_demo\"]}')\n",
    "         #cprint.warn(f'Model Loss: {loss[\"loss_demo\"]}')\n",
    "     loss = model.get_loss()\n",
    "     cprint.ok(f'Running Loss end of epoch {epoch}: {runningloss}')   \n",
    "#      cprint.ok(f'Model Loss end of epoch {epoch}: {loss[\"loss\"]}')\n",
    "#      cprint.ok(f'Model Demo Loss end of epoch {epoch}: {loss[\"loss_demo\"]}')\n",
    "     runningloss=0\n",
    "     model.scheduler.step()\n",
    "#      if(epoch%10==0):\n",
    "#          torch.save(model.state_dict(), f\"{experiment_dir}/checkpoints/epoch-{epoch}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizations import visualize_occupancy,visualize_images\n",
    "from utils.util import iou\n",
    "sig = torch.nn.Sigmoid()\n",
    "with torch.no_grad():\n",
    "    z_prev = None\n",
    "    for i in range(30):\n",
    "        data = dataset[i]\n",
    "        data['images'] = torch.tensor(data['images']).unsqueeze(0).float().cuda()\n",
    "        data['voxels'] = torch.tensor(data['voxels']).float().cuda()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        tgt = data['voxels']\n",
    "        \n",
    "        pred = model(data)\n",
    "        \n",
    "        #import pdb;pdb.set_trace()\n",
    "        pred = pred.squeeze(0).clone()\n",
    "        pred = sig(pred)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        #pred\n",
    "        #print(iou(tgt.unsqueeze(0),pred.unsqueeze(0),0.4))\n",
    "        if(z_prev is not None):\n",
    "            #import pdb;pdb.set_trace()\n",
    "            #print(\"With Previous\",iou(z_prev,pred.unsqueeze(0),0.4))\n",
    "            print(\"With Target:\",iou(tgt.unsqueeze(0),pred.unsqueeze(0),0.4))\n",
    "        z_prev = pred.unsqueeze(0).clone()\n",
    "#         pred[pred>=0.4] = 1\n",
    "#         pred[pred<0.4] = 0\n",
    "        torch.save(pred, f\"{experiment_dir}/visuals/pred_{i}.pt\")\n",
    "        #print(\"With Target:\",iou(tgt.unsqueeze(0),pred.unsqueeze(0),0.4))\n",
    "        #visualize_occupancy(tgt.detach().squeeze(0).cpu().numpy(),flip_axes=True)\n",
    "        cprint.ok(\"=============================================================\")\n",
    "        #visualize_occupancy(pred.detach().squeeze(0).cpu().numpy(),flip_axes=True)\n",
    "        #plot_voxels(pred.detach(), rot02=1, rot12=1)\n",
    "        #import pdb;pdb.set_trace()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "def plot_voxels(voxels, rot01=0, rot02=0, rot12=0):\n",
    "    voxels = voxels[0]\n",
    "    voxels[voxels >= 0.5] = 1\n",
    "    voxels[voxels < 0.5] = 0\n",
    "    voxels = voxels.rot90(rot01, (0, 1))\n",
    "    voxels = voxels.rot90(rot02, (0, 2))\n",
    "    voxels = voxels.rot90(rot12, (1, 2))\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    ax.set_box_aspect((1, 1, 1))\n",
    "    ax.voxels(voxels)\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y4 = torch.load(f\"{experiment_dir}/visuals/pred_20.pt\")\n",
    "y5 = torch.load(f\"{experiment_dir}/visuals/pred_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c706ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred, gt):\n",
    "    pred = pred.clone()\n",
    "    gt = gt.clone()\n",
    "    gt[gt<=0.4] = 0\n",
    "    gt[gt>=0.4] = 1\n",
    "    pred[pred <= 0.4] = 0\n",
    "    pred[pred >= 0.4] = 1\n",
    "    print((pred!=gt).sum())\n",
    "    intersection = torch.sum(pred.mul(gt)).float()\n",
    "    union = torch.sum(torch.ge(pred.add(gt), 1)).float()\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_iou(y4,y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs((y4-y5)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voxels(y5.detach(), rot02=1, rot12=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f064a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_occupancy(y4.detach().squeeze(0).cpu().numpy(),flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e93ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ceb326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
