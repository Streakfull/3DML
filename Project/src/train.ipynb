{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from utils.util import mkdir,seed_all\n",
    "from omegaconf import OmegaConf\n",
    "from cprint import *\n",
    "from datasets.shape_net import ShapeNet\n",
    "import torch\n",
    "from models.Transform2D import Transform2D\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Expirement Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m{'logs_dir': 'logs', 'is_train': True, 'name': 'testingOverfit', 'device': 'cuda:0', 'batch_size': 8, 'n_epochs': 500, 'print_every': 300, 'validate_every': 300, 'model': {'lr': 0.0001}}\u001b[0m\n",
      "\u001b[94m- logs directory found\u001b[0m\n",
      "\u001b[94m- logs/testingOverfit directory found\u001b[0m\n",
      "\u001b[94m- logs/testingOverfit/checkpoints directory found\u001b[0m\n",
      "\u001b[94m- logs/testingOverfit/tb directory found\u001b[0m\n",
      "\u001b[94m- logs/testingOverfit/visuals directory found\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "seed_all(111)\n",
    "config = OmegaConf.load(\"./configs/global_configs.yaml\")\n",
    "cprint.ok(config)\n",
    "description = \"Testing Overfit\" # Describe Experiment params here\n",
    "logs_dir = config[\"logs_dir\"]\n",
    "mkdir(logs_dir)\n",
    "\n",
    "experiment_dir = f\"{logs_dir}/{config['name']}\"\n",
    "mkdir(experiment_dir)\n",
    "mkdir(f\"{experiment_dir}/checkpoints\")\n",
    "mkdir(f\"{experiment_dir}/tb\")\n",
    "mkdir(f\"{experiment_dir}/visuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & Dataloaders\n",
    "This uses a random split for train/validation/test - might need to look into paper if they have a pre-defined split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4045"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ShapeNet(cat=\"airplane\",is_overfit=False) #Change overfit param here & cat here\n",
    "# train_ds, valid_ds, test_ds = torch.utils.data.random_split(\n",
    "#     dataset, [64, 0, 0])\n",
    "train_ds = dataset\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "len(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mUsing device:\u001b[0m cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transform2D(\n",
       "  (encoder): Encoder(\n",
       "    (conv1a): ConvBlock(\n",
       "      (conv): Conv2d(4, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv1b): ConvBlock(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv2a): ConvBlock(\n",
       "      (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv2b): ConvBlock(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (res2): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv3a): ConvBlock(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv3b): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (res3): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (pool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv4a): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv4b): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (pool4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv5a): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv5b): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (res5): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (pool5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv6a): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv6b): ConvBlock(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (res6): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (pool6): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc): Linear(in_features=6400, out_features=1024, bias=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (sequential): Sequential()\n",
       "    (net): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (linear1tf): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (net2): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear2): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (linear2tf): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (linear3): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "    (linear3tf): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "    (linear4): Sequential(\n",
       "      (0): Linear(in_features=8192, out_features=12288, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=12288, out_features=16384, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (initialpool): MaxPool3d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1a): ConvBlock(\n",
       "      (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv1b): ConvBlock(\n",
       "      (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (res1): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (unpool1): MaxUnpool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (conv2a): ConvBlock(\n",
       "      (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2b): ConvBlock(\n",
       "      (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (res2): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (unpool2): MaxUnpool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (conv3a): ConvBlock(\n",
       "      (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3b): ConvBlock(\n",
       "      (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (res3): ResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (unpool3): MaxUnpool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (conv4a): ConvBlock(\n",
       "      (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4b): ConvBlock(\n",
       "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4c): ConvBlock(\n",
       "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (res4): TripleResidualBlock(\n",
       "      (conv1): ConvBlock(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv2): ConvBlock(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (conv3): ConvBlock(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (convi): ConvBlock(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (unpool4): MaxUnpool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(2, 2, 2))\n",
       "    (conv5a): ConvBlock(\n",
       "      (conv): Conv3d(32, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (relu): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (criterion_demo): BCELoss()\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transform2D()\n",
    "# Declare device\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available() and config['device'].startswith('cuda'):\n",
    "    device = torch.device(config['device'])\n",
    "    cprint.ok('Using device:', config['device'])\n",
    "else:\n",
    "    cprint.warn('Using CPU')\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Checklist:\n",
    "- Add tensorboard and make sure it logs to the `${experiment_dir}/tb` folder\n",
    "- Visualize some reconstructions on validation set and make sure it logs to the `${expirement_dir}/visuals` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605606a3d562449cb73b23932ca17f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb22a5f359c44978ac79ce5a72bcb900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youssef/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry_final_pro-8w_t9tN0-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:847: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool3d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transform2D' object has no attribute 'get_current_errors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47409/278053700.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mShapeNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_batch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           \u001b[0mtrain_loss_running\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m           \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_every'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_every'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry_final_pro-8w_t9tN0-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transform2D' object has no attribute 'get_current_errors'"
     ]
    }
   ],
   "source": [
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(config['n_epochs'])):\n",
    "     for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "          ShapeNet.move_batch_to_device(batch, device)\n",
    "          model.step(batch)\n",
    "          train_loss_running += model.get_current_errors()\n",
    "          iteration = epoch * len(train_dataloader) + batch_idx\n",
    "          if iteration % config['print_every'] == (config['print_every'] - 1):\n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] train_loss: {train_loss_running / config[\"print_every_n\"]:.6f}')\n",
    "            train_loss_running = 0.\n",
    "\n",
    "\n",
    "     if iteration % config['validate_every'] == (config['validate_every'] - 1 or True):\n",
    "          model.eval()\n",
    "          loss_val = 0.\n",
    "          for batch_val in validation_dataloader:\n",
    "               ShapeNet.move_batch_to_device(batch_val, device)\n",
    "               with torch.no_grad():\n",
    "                   model.set_input(batch_val)\n",
    "                   output = model.inference()\n",
    "                   loss_val += model.get_current_errors()\n",
    "          \n",
    "          if loss_val < best_loss_val:\n",
    "                    model.save()\n",
    "                    best_loss_val = loss_val\n",
    "\n",
    "          cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] val_loss: {loss_val:.6f} | best_loss_val: {best_loss_val:.6f}')\n",
    "     model.update_lr()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on  single instance for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "data = dataset[0]\n",
    "#data[\"images\"] = data[\"images\"].squeeze(0)\n",
    "data[\"images\"] = torch.tensor(data[\"images\"]).cuda().float()\n",
    "data[\"voxels\"] = torch.tensor(data[\"voxels\"]).cuda().float()\n",
    "for epoch in tqdm(range(800)):\n",
    "    model.step(data)\n",
    "    loss = model.get_loss()\n",
    "    cprint.ok(f'Model Loss: {loss[\"loss\"]}')\n",
    "    cprint.warn(f'Model Loss: {loss[\"loss_demo\"]}')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(160)):\n",
    "     for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "          ShapeNet.move_batch_to_device(batch, device)\n",
    "          model.step(batch)\n",
    "         #cprint.warn(f'Model Loss: {loss[\"loss_demo\"]}')\n",
    "     loss = model.get_loss()\n",
    "     cprint.ok(f'Model Loss: {loss[\"loss\"]}')\n",
    "#      if(epoch%10==0):\n",
    "#          torch.save(model.state_dict(), f\"{experiment_dir}/checkpoints/epoch-{epoch}.ckpt\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)\n",
    "data  = train_ds[39]\n",
    "\n",
    "data['voxels'].shape\n",
    "data['images'].shape\n",
    "#data= dataset[1]\n",
    "data['images'].shape\n",
    "data['voxels'].shape\n",
    "data['images'] = data['images'][np.newaxis,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['images'] = torch.tensor(data['images']).float().cuda()\n",
    "data['voxels'] = torch.tensor(data['voxels']).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizations import visualize_occupancy,visualize_images\n",
    "data['voxels'].shape\n",
    "tgt = data['voxels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_occupancy(data[\"voxels\"].squeeze(0).cpu(),flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape\n",
    "sig = torch.nn.Sigmoid()\n",
    "probs = sig(x)\n",
    "\n",
    "# probs.shape\n",
    "# probs.shape\n",
    "# pred = probs[8]\n",
    "\n",
    "# pred[pred>0.].shape\n",
    "# pred[pred<0.3] = 0\n",
    "# pred[pred>=0.1] = 1\n",
    "# # pred.shape\n",
    "\n",
    "# tgt[tgt!=0].shape\n",
    "# pred[pred!=0].shape\n",
    "# pred\n",
    "# x.shape\n",
    "# probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape\n",
    "pred = probs.clone()\n",
    "pred[pred<0.4] = 0\n",
    "pred[pred>=0.4] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_occupancy(pred.detach().squeeze(0).cpu().numpy(),flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import iou\n",
    "tgt.shape\n",
    "#pred.shape\n",
    "#probs.max(dim=1)\n",
    "iou(tgt.unsqueeze(0),probs.unsqueeze(0),0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape\n",
    "#tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = probs.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "inter = rearrange(inter, '(c1 c2 c3) -> c1 c2 c3',c1=32,c2=32,c3=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter[inter>0.4] = 1\n",
    "inter[inter<=0.4] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_occupancy(inter.detach().squeeze(0).cpu().numpy(),flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2(probs,tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import iou\n",
    "sig = torch.nn.Sigmoid()\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        data = dataset[i]\n",
    "        data['images'] = torch.tensor(data['images']).unsqueeze(0).float().cuda()\n",
    "        data['voxels'] = torch.tensor(data['voxels']).float().cuda()\n",
    "        #import pdb;pdb.set_trace()\n",
    "        tgt = data['voxels']\n",
    "        pred = model(data)\n",
    "        #pred = pred.squeeze(0)\n",
    "        pred = sig(pred)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        print(iou(tgt.unsqueeze(0),pred.unsqueeze(0),0.4))\n",
    "        pred[pred>=0.4] = 1\n",
    "        pred[pred<0.4] = 0\n",
    "        #visualize_occupancy(tgt.detach().squeeze(0).cpu().numpy(),flip_axes=True)\n",
    "        cprint.ok(\"=============================================================\")\n",
    "        #visualize_occupancy(pred.detach().squeeze(0).cpu().numpy(),flip_axes=True)\n",
    "        #import pdb;pdb.set_trace()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
