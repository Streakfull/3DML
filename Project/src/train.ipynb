{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import mkdir,seed_all\n",
    "from omegaconf import OmegaConf\n",
    "from cprint import *\n",
    "from datasets.shape_net import ShapeNet\n",
    "import torch\n",
    "from models.base_model import BaseModel\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Expirement Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m{'logs_dir': 'logs', 'is_train': True, 'name': 'setup3', 'device': 'cuda:0', 'batch_size': 16, 'n_epochs': 1, 'print_every': 50, 'validate_every': 50}\u001b[0m\n",
      "\u001b[94m- logs directory found\u001b[0m\n",
      "\u001b[94m- logs/setup3 directory found\u001b[0m\n",
      "\u001b[94m- logs/setup3/checkpoints directory found\u001b[0m\n",
      "\u001b[94m- logs/setup3/tb directory found\u001b[0m\n",
      "\u001b[94m- logs/setup3/visuals directory found\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "seed_all(111)\n",
    "config = OmegaConf.load(\"./configs/global_configs.yaml\")\n",
    "cprint.ok(config)\n",
    "description = \"Testing training script\" # Describe Experiment params here\n",
    "logs_dir = config[\"logs_dir\"]\n",
    "mkdir(logs_dir)\n",
    "\n",
    "experiment_dir = f\"{logs_dir}/{config['name']}\"\n",
    "mkdir(experiment_dir)\n",
    "mkdir(f\"{experiment_dir}/checkpoints\")\n",
    "mkdir(f\"{experiment_dir}/tb\")\n",
    "mkdir(f\"{experiment_dir}/visuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & Dataloaders\n",
    "This uses a random split for train/validation/test - might need to look into paper if they have a pre-defined split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  4045\n"
     ]
    }
   ],
   "source": [
    "dataset = ShapeNet(cat=\"airplane\",is_overfit=False) #Change overfit param here & cat here\n",
    "print('length: ', len(dataset))\n",
    "train_ds, valid_ds, test_ds = torch.utils.data.random_split(\n",
    "    dataset, [2830, 810, 405])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mUsing device:\u001b[0m cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "# Declare device\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available() and config['device'].startswith('cuda'):\n",
    "    device = torch.device(config['device'])\n",
    "    cprint.ok('Using device:', config['device'])\n",
    "else:\n",
    "    cprint.warn('Using CPU')\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Checklist:\n",
    "- Add tensorboard and make sure it logs to the `${experiment_dir}/tb` folder\n",
    "- Visualize some reconstructions on validation set and make sure it logs to the `${expirement_dir}/visuals` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 16:40:35.778500: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-15 16:40:35.823817: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-15 16:40:37.828244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cedd0f1e22841788fb90883e01bb637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5b3d94016e49f0b4cd653c018f546a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m[000/00049] train_loss: 0.000000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINGO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m[000/00049] val_loss: 0.000000 | best_loss_val: 0.000000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m[000/00099] train_loss: 0.000000\u001b[0m\n",
      "\u001b[93m[000/00099] val_loss: 0.000000 | best_loss_val: 0.000000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m[000/00149] train_loss: 0.000000\u001b[0m\n",
      "\u001b[93m[000/00149] val_loss: 0.000000 | best_loss_val: 0.000000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mVisualizations saved to: logs/setup3/visuals\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "\n",
    "tb_dir = f\"{experiment_dir}/tb\"\n",
    "writer = SummaryWriter(log_dir=tb_dir)\n",
    "\n",
    "#print(experiment_dir)\n",
    "#print(os.listdir(f\"{experiment_dir}/visuals\"))\n",
    "\n",
    "for epoch in tqdm(range(config['n_epochs'])):\n",
    "    for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "        ShapeNet.move_batch_to_device(batch, device)\n",
    "        model.set_input(batch)\n",
    "        model.step()\n",
    "        train_loss_running += model.get_current_errors()\n",
    "        iteration = epoch * len(train_dataloader) + batch_idx\n",
    "        #print(iteration)\n",
    "        if iteration % config['print_every'] == (config['print_every'] - 1):\n",
    "            avg_train_loss = train_loss_running / config[\"print_every\"]\n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] train_loss: {avg_train_loss:.6f}')\n",
    "            writer.add_scalar(\"Train Loss\", avg_train_loss, iteration)\n",
    "            # cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] train_loss: {train_loss_running / config[\"print_every_n\"]:.6f}')\n",
    "            train_loss_running = 0.\n",
    "\n",
    "        if iteration % config['validate_every'] == (config['validate_every'] - 1 or True):\n",
    "            model.eval()\n",
    "            loss_val = 0.\n",
    "            output = []\n",
    "            \n",
    "            for batch_val in validation_dataloader:\n",
    "                ShapeNet.move_batch_to_device(batch_val, device)\n",
    "                with torch.no_grad():\n",
    "                    model.set_input(batch_val)\n",
    "                    output = model.inference()\n",
    "                    loss_val += model.get_current_errors()\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "                model_path = f\"{experiment_dir}/checkpoints/model_{epoch:03d}.pt\"\n",
    "                model.save(model_path)\n",
    "                best_loss_val = loss_val\n",
    "\n",
    "            # cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] val_loss: {loss_val:.6f} | best_loss_val: {best_loss_val:.6f}')\n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] val_loss: {loss_val:.6f} | best_loss_val: {best_loss_val:.6f}')\n",
    "            writer.add_scalar(\"Validation Loss\", loss_val, iteration)\n",
    "\n",
    "            #mkdir(f\"{experiment_dir}/visuals\")\n",
    "\n",
    "#             for i in range(len(output)):\n",
    "#                 visualization = output[i].detach().cpu().numpy()\n",
    "#                 visualization_path = f\"{experiment_dir}/visuals/reconstruction_{epoch:03d}_{i:05d}.png\"\n",
    "#                 visualization = np.transpose(visualization, (1, 2, 0))\n",
    "#                 visualization = ((visualization + 1) / 2) * 255  # Convert [-1, 1] to [0, 255]\n",
    "#                 visualization = visualization.astype(np.uint8)\n",
    "#                 image = Image.fromarray(visualization)\n",
    "#                 image.save(visualization_path)\n",
    "                \n",
    "            \n",
    "cprint.ok(f\"Visualizations saved to: {experiment_dir}/visuals\")\n",
    "\n",
    "model.update_lr()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
