{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5f3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import mkdir,seed_all\n",
    "from omegaconf import OmegaConf\n",
    "from cprint import *\n",
    "from datasets.shape_net import ShapeNet\n",
    "import torch\n",
    "from models.Transform2D import Transform2D\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.visualizations import save_voxels\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb237d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m{'logs_dir': 'logs', 'is_train': True, 'name': 'with_img_transforms_plz_work', 'device': 'cuda:0', 'batch_size': 16, 'n_epochs': 100, 'print_every': 150, 'validate_every': 3000, 'save_every': 3000, 'save_every_nepochs': 10, 'model': {'lr': 0.0001, 'criterion': 'BCE', 'pos_weight': 1.3, 'encoder': {'patch_size': 16, 'sequence_length': 196, 'embedding_dim': 768, 'patch_padding': 3}, 'transformer_encoder': {'d_model': 768, 'nhead': 12, 'num_layers': 12}, 'transformer_decoder': {'d_model': 768, 'nhead': 12, 'num_layers': 8, 'num_pos_embeddings': 64}}}\u001b[0m\n",
      "\u001b[94m- logs directory found\u001b[0m\n",
      "\u001b[94m- logs/testtt directory found\u001b[0m\n",
      "\u001b[94m- logs/testtt/checkpoints directory found\u001b[0m\n",
      "\u001b[94m- logs/testtt/tb directory found\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "seed_all(111)\n",
    "today = time.strftime(\"%Y-%m-%d\")\n",
    "config = OmegaConf.load(\"./configs/global_configs.yaml\")\n",
    "cprint.ok(config)\n",
    "description = \"all dataset gg\" # Describe Experiment params here\n",
    "logs_dir = config[\"logs_dir\"]\n",
    "mkdir(logs_dir)\n",
    "experiment_dir = f\"{logs_dir}/testtt\"\n",
    "mkdir(experiment_dir)\n",
    "loss_log_title = \"Loss Log\" + today \n",
    "\n",
    "with open(f\"{experiment_dir}/description.txt\", \"w\") as file1:\n",
    "    file1.write(description)\n",
    "    \n",
    "with open(f\"{experiment_dir}/configs.txt\", \"w\") as file1:\n",
    "    file1.write(str(config))\n",
    "\n",
    "with open(f\"{experiment_dir}/loss_log.txt\", \"w\") as file1:\n",
    "    file1.write(loss_log_title)\n",
    "    file1.write(\"\\n\")\n",
    "\n",
    "\n",
    "mkdir(f\"{experiment_dir}/checkpoints\")\n",
    "mkdir(f\"{experiment_dir}/tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b0d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  6778\n"
     ]
    }
   ],
   "source": [
    "dataset = ShapeNet(cat=\"chair\",is_overfit=False, nimgs=3) #Change overfit param here & cat here\n",
    "print('length: ', len(dataset))\n",
    "config = OmegaConf.load(\"./configs/global_configs.yaml\")\n",
    "dataset[0]\n",
    "train_ds, valid_ds, test_ds = torch.utils.data.random_split(\n",
    "    dataset, [6000, 778, 0])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        #num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=config['batch_size'],   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        #num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "        # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, DeiTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301acea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeiTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16601/3677642803.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeiTModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/deit-base-distilled-patch16-224\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DeiTModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = DeiTModel.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce43d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed32c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing DeiTModel: ['cls_classifier.weight', 'cls_classifier.bias', 'distillation_classifier.bias', 'distillation_classifier.weight']\n",
      "- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['deit.pooler.dense.bias', 'deit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Transform2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1d9e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ef720ede5b47b29cf813a15f753e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4e84b00c00403d80005ddc11e2cf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/hdd/streakfull/3DML/Project/src/models/Transform2D.py\u001b[0m(95)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     93 \u001b[0;31m        \u001b[0;31m#import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> self.images.shape\n",
      "torch.Size([48, 3, 224, 224])\n",
      "ipdb> k = self.deit_model(self.images)\n",
      "ipdb> k.shape\n",
      "*** AttributeError: 'BaseModelOutputWithPooling' object has no attribute 'shape'\n",
      "ipdb> last_hidden_states = k.last_hidden_state\n",
      "ipdb> last_hidden_states.shape\n",
      "torch.Size([48, 198, 768])\n",
      "ipdb> last_hidden_states\n",
      "tensor([[[ 2.0033e-01,  1.3465e-01,  7.0917e-02,  ..., -5.1942e-02,\n",
      "          -3.3614e-01, -8.1730e-01],\n",
      "         [ 8.3741e-01,  5.3440e-01, -1.2683e+00,  ..., -3.1051e-01,\n",
      "          -1.1625e+00, -3.5187e-01],\n",
      "         [-2.5050e-02,  5.7442e-02, -8.7433e-01,  ..., -3.1038e-01,\n",
      "          -4.5043e-01, -4.1194e-03],\n",
      "         ...,\n",
      "         [ 4.6494e-01,  5.7515e-01, -6.7269e-01,  ..., -7.2884e-01,\n",
      "          -3.8007e-01, -2.2977e-01],\n",
      "         [ 7.5545e-01,  5.3968e-01, -7.6257e-01,  ..., -4.5074e-01,\n",
      "           1.6260e-01, -2.8184e-01],\n",
      "         [ 4.9283e-01,  2.0347e-01, -1.0158e+00,  ..., -5.6791e-01,\n",
      "          -1.0786e+00, -4.3952e-01]],\n",
      "\n",
      "        [[ 6.1914e-01, -1.9614e-03, -9.3621e-02,  ...,  5.3162e-01,\n",
      "          -4.5509e-01, -8.7450e-01],\n",
      "         [ 1.5349e+00,  6.3996e-02, -1.3604e+00,  ...,  9.2945e-01,\n",
      "          -1.3204e+00, -2.7442e-01],\n",
      "         [ 3.5208e-01,  2.0607e-01, -7.4168e-01,  ...,  6.9863e-02,\n",
      "          -5.9377e-01,  3.6183e-02],\n",
      "         ...,\n",
      "         [ 6.1604e-01,  6.2708e-01, -6.3557e-01,  ..., -6.5031e-01,\n",
      "          -6.2062e-01, -2.8313e-01],\n",
      "         [ 9.7114e-01,  4.2740e-01, -7.3606e-01,  ..., -2.8467e-01,\n",
      "           1.0764e-01, -3.2939e-01],\n",
      "         [ 9.2636e-01,  4.0028e-01, -1.0261e+00,  ..., -1.7383e-01,\n",
      "          -1.1553e+00, -2.5104e-01]],\n",
      "\n",
      "        [[ 5.2136e-02,  2.6844e-01,  1.2347e-01,  ...,  7.4993e-02,\n",
      "          -2.4586e-01, -7.6904e-01],\n",
      "         [ 3.0170e-01,  8.0346e-01, -8.9056e-01,  ..., -1.6899e-01,\n",
      "          -8.2858e-01, -1.4868e-01],\n",
      "         [-2.1442e-01,  1.7418e-01, -7.5463e-01,  ...,  3.2520e-03,\n",
      "           7.1182e-02,  3.8458e-02],\n",
      "         ...,\n",
      "         [ 4.3384e-01,  6.9556e-01, -5.5773e-01,  ..., -8.1172e-01,\n",
      "          -2.9191e-01, -4.1775e-01],\n",
      "         [ 7.3094e-01,  6.3645e-01, -5.7334e-01,  ..., -3.4321e-01,\n",
      "           3.1606e-01, -4.1007e-01],\n",
      "         [ 3.4082e-01,  4.0306e-01, -9.2681e-01,  ..., -1.3496e-01,\n",
      "          -7.7718e-01, -3.7481e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2529e-01, -1.2193e-01, -8.5075e-01,  ..., -7.2802e-01,\n",
      "          -1.9983e-01,  5.9309e-01],\n",
      "         [ 7.8435e-01, -9.8093e-02, -1.2854e+00,  ..., -7.6986e-01,\n",
      "          -3.9689e-01,  1.3921e+00],\n",
      "         [ 1.3846e+00,  1.3723e+00, -1.2505e+00,  ..., -4.7671e-02,\n",
      "          -8.5228e-03, -1.6964e-01],\n",
      "         ...,\n",
      "         [-4.5193e-02, -1.8176e-03, -9.7258e-01,  ..., -5.6480e-01,\n",
      "           8.0126e-02,  3.1363e-01],\n",
      "         [ 3.6749e-01,  9.2844e-02, -9.8959e-01,  ..., -4.6163e-01,\n",
      "           4.3537e-01,  2.4196e-01],\n",
      "         [ 9.7516e-01,  1.0240e+00, -2.1336e+00,  ...,  2.0448e-01,\n",
      "          -6.1415e-01,  1.9161e-01]],\n",
      "\n",
      "        [[-7.3060e-02, -3.4472e-01, -6.3410e-01,  ..., -2.3793e-01,\n",
      "          -1.5601e-01,  3.8816e-01],\n",
      "         [ 7.8588e-01, -1.3956e-01, -1.5601e+00,  ..., -6.9223e-02,\n",
      "          -5.6577e-01,  1.1721e+00],\n",
      "         [ 7.9256e-01,  8.3779e-01, -1.4776e+00,  ...,  3.1978e-01,\n",
      "          -5.2471e-01,  5.9809e-01],\n",
      "         ...,\n",
      "         [ 3.5530e-01,  1.6124e-01, -7.8985e-01,  ..., -7.3826e-01,\n",
      "          -2.1966e-01,  3.1143e-01],\n",
      "         [ 6.5878e-01,  7.0445e-02, -7.9060e-01,  ..., -4.9024e-01,\n",
      "           2.9066e-01,  1.5428e-01],\n",
      "         [ 1.0442e+00,  5.0298e-01, -1.6752e+00,  ...,  1.6134e-01,\n",
      "          -1.1048e+00,  3.1338e-01]],\n",
      "\n",
      "        [[ 1.4611e-01,  2.8569e-01, -4.0890e-01,  ..., -4.8571e-01,\n",
      "          -1.9105e-01,  5.6919e-01],\n",
      "         [ 1.3238e+00,  8.3160e-01, -1.0868e+00,  ..., -5.8689e-01,\n",
      "          -8.5255e-01,  1.5711e+00],\n",
      "         [ 8.9139e-01,  8.7483e-01, -1.0621e-01,  ...,  2.9095e-01,\n",
      "           8.9319e-02,  6.7233e-01],\n",
      "         ...,\n",
      "         [ 2.7671e-01,  1.5708e-01, -7.0887e-01,  ..., -3.7268e-01,\n",
      "          -7.5859e-02,  4.2229e-01],\n",
      "         [ 5.4033e-01,  1.7264e-01, -7.4277e-01,  ..., -3.1121e-01,\n",
      "           3.7914e-01,  2.5344e-01],\n",
      "         [ 1.1894e+00,  5.9332e-01, -1.4888e+00,  ...,  1.4225e-01,\n",
      "          -8.7872e-01,  4.9143e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "from utils.visualizations import visualize_png\n",
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "\n",
    "tb_dir = f\"{experiment_dir}/tb\"\n",
    "writer = SummaryWriter(log_dir=tb_dir)\n",
    "model_checkpoint_path = f\"{experiment_dir}/checkpoints\"\n",
    "loss_log_name = f\"{experiment_dir}/loss_log.txt\"\n",
    "visuals_path =  f\"{experiment_dir}/visuals\"\n",
    "last_loss = 0.\n",
    "last_iou = 0.\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, writer):\n",
    "     global best_loss_val\n",
    "     global last_loss   \n",
    "     global last_iou\n",
    "     train_loss_running = 0.\n",
    "     train_iou_running = 0.\n",
    "     iteration_count = 0\n",
    "     for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "         ShapeNet.move_batch_to_device_float(batch, \"ss\")\n",
    "         model.step(batch)\n",
    "         metrics = model.get_metrics()\n",
    "         loss = metrics[\"loss\"]\n",
    "         iou = metrics[\"iou\"]\n",
    "         train_loss_running += loss\n",
    "         train_iou_running += iou\n",
    "         iteration = epoch * len(train_dataloader) + batch_idx   \n",
    "         iteration_count += 1\n",
    "         if iteration % config['print_every'] == (config['print_every'] - 1):\n",
    "            avg_train_loss = train_loss_running / iteration_count\n",
    "            avg_iou = train_iou_running / iteration_count\n",
    "            message = '(epoch: %d, iters: %d, loss: %.6f, iou: %.6f)' % (epoch, iteration, loss.item(), iou.item())\n",
    "            with open(loss_log_name, \"a\") as log_file:\n",
    "                log_file.write('%s\\n' % message)\n",
    "            print(loss)\n",
    "            reconstructions = model.x\n",
    "            target = batch[\"voxels\"].squeeze(1)\n",
    "            fig = save_voxels(reconstructions, target, visuals_path, iteration, is_train=True )\n",
    "            writer.add_figure(\"Train/Reconstructions\", fig, global_step=iteration, close=True, walltime=None)\n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] train_loss: {avg_train_loss:.6f}')\n",
    "            writer.add_scalar(\"Train/Loss\", avg_train_loss, iteration)\n",
    "            writer.add_scalar(\"Train/iou\", avg_iou, iteration)\n",
    "            last_loss = avg_train_loss\n",
    "            last_iou = avg_iou\n",
    "            train_loss_running = 0.\n",
    "            train_iou_running = 0.\n",
    "            iteration_count = 0\n",
    "         \n",
    "         if iteration % config['save_every'] == (config['save_every'] - 1):\n",
    "            model.save(model_checkpoint_path, \"latest\")\n",
    "            \n",
    "        \n",
    "         if iteration % config['validate_every'] == (config['validate_every'] - 1):\n",
    "            cprint.ok(\"Running Validation\")\n",
    "            model.eval()\n",
    "            loss_val = 0.\n",
    "            iou_val = 0.\n",
    "            index_batch = 0\n",
    "            for batch_val in validation_dataloader:\n",
    "                ShapeNet.move_batch_to_device_float(batch_val, device)\n",
    "                with torch.no_grad():\n",
    "                    model.inference(batch_val)\n",
    "                    metrics = model.get_metrics()\n",
    "                    loss_val +=  metrics[\"loss\"]\n",
    "                    iou_val += metrics[\"iou\"]\n",
    "                    index_batch += 1\n",
    "            avg_loss_val = loss_val / (index_batch)\n",
    "            avg_iou_val = iou_val / (index_batch)\n",
    "            reconstructions = model.x\n",
    "            target = batch_val[\"voxels\"].squeeze(1)\n",
    "            fig = save_voxels(reconstructions, target, visuals_path, iteration, is_train=False )\n",
    "            writer.add_figure(\"Validation/Reconstructions\", fig, global_step=iteration, close=True, walltime=None)\n",
    "            \n",
    "            if avg_loss_val < best_loss_val:\n",
    "                model.save(model_checkpoint_path, \"best\")\n",
    "                best_loss_val = avg_loss_val\n",
    "            \n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] val_loss: {avg_loss_val:.6f} | best_loss_val: {best_loss_val:.6f}')\n",
    "            writer.add_scalar(\"Validation/Loss\", avg_loss_val, iteration)\n",
    "            #import pdb;pdb.set_trace();\n",
    "            writer.add_scalars('Validation/LossComparison',\n",
    "                    { 'Training' : last_loss, 'Validation' : avg_loss_val },\n",
    "                     iteration)\n",
    "            \n",
    "            writer.add_scalars(\"Validation/iouComparison\",\n",
    "                                 { 'Training' : last_iou, 'Validation' : avg_iou_val},\n",
    "                                     iteration)\n",
    "            \n",
    "            writer.flush()\n",
    "      \n",
    "            return last_loss\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(config['n_epochs'])):\n",
    "    avg_loss = train_one_epoch(epoch, writer) \n",
    "    if(epoch % config[\"save_every_nepochs\"]==0):\n",
    "        model.save(model_checkpoint_path, epoch)\n",
    "    model.update_lr()\n",
    "    writer.close()\n",
    "            \n",
    "#cprint.ok(f\"Visualizations saved to: {experiment_dir}/visuals\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52aab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
